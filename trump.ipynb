{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPm4mCflJz3w8Tytx29Umic",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/worldterminator/mess/blob/main/trump.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nLPrlvejP2eU",
        "outputId": "015b772b-e5f3-4cc8-cf18-0aed06d565ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "   Durationinseconds                                      describephoto  \\\n",
            "0                 30                                                NaN   \n",
            "1                142                                            rioting   \n",
            "2                243   Terrorist acts against a duly elected government   \n",
            "3                162        People are trying to overturn our democracy   \n",
            "4                282  White supremacists under the guise of Qanon ar...   \n",
            "\n",
            "                                          whynervous feedback     IP_country  \\\n",
            "0                                                NaN      NaN  United States   \n",
            "1         Because it shows disregard for rule of law      NaN  United States   \n",
            "2  Our president is encouraging a coup and has un...      NaN  United States   \n",
            "3  I think they're trying to disregard the basis ...      NaN  United States   \n",
            "4  It is worrying that the president has encourag...      NaN  United States   \n",
            "\n",
            "   ID  \n",
            "0  66  \n",
            "1  67  \n",
            "2  68  \n",
            "3  69  \n",
            "4  70  \n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import pandas as pd\n",
        "file_path = '/content/drive/My Drive/trump.csv'\n",
        "trump = pd.read_csv(file_path)\n",
        "\n",
        "print(trump.head())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming 'data' is your DataFrame\n",
        "# Fill NaN in 'whynervous' with a placeholder or drop\n",
        "data['whynervous'].fillna('No Response', inplace=True)  # Placeholder example\n",
        "# data.dropna(subset=['whynervous'], inplace=True)  # Dropping rows without a response\n",
        "\n",
        "# Basic text cleaning (lowercasing, removing punctuation)\n",
        "data['whynervous_clean'] = data['whynervous'].str.lower().str.replace('[^\\w\\s]', '', regex=True)\n"
      ],
      "metadata": {
        "id": "JZM4ZQ2-XXa2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Flagging short responses\n",
        "min_words = 3\n",
        "data['flag_invalid'] = data['whynervous_clean'].apply(lambda x: len(x.split()) < min_words)\n",
        "\n",
        "# Additional heuristics can be added similarly"
      ],
      "metadata": {
        "id": "FrNGH1Z9ZLAJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download necessary NLTK data (do this once)\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Tokenizing responses\n",
        "data['whynervous_tokens'] = data['whynervous_clean'].apply(word_tokenize)\n",
        "\n",
        "# Example of further analysis: Counting tokens in each response\n",
        "data['token_count'] = data['whynervous_tokens'].apply(len)\n",
        "# Filtering rows where the token count is 3 or more\n",
        "data_filtered = data[data['token_count'] >= 3]\n"
      ],
      "metadata": {
        "id": "uZEZ1CNAZstz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "removing stop words might be less necessary for flagging bad responses but could help preparing the filtered data for later analysis...codes that include re"
      ],
      "metadata": {
        "id": "oBVS_nvWacNp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Set of English stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Function to tokenize and clean text (remove punctuation and stop words)\n",
        "def clean_tokenize(text):\n",
        "    # Removing punctuation\n",
        "    no_punctuation = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    # Tokenizing\n",
        "    tokens = word_tokenize(no_punctuation)\n",
        "    # Removing stop words and lowercasing\n",
        "    return [word.lower() for word in tokens if word.lower() not in stop_words]\n",
        "\n",
        "# Applying the cleaning and tokenization function\n",
        "data['whynervous_tokens_clean'] = data['whynervous_clean'].apply(clean_tokenize)\n",
        "\n",
        "# Counting tokens again (after removing stop words and punctuation)\n",
        "data['token_count_clean'] = data['whynervous_tokens_clean'].apply(len)\n",
        "\n",
        "# Filtering rows based on the new token count\n",
        "data_filtered = data[data['token_count_clean'] >= 3]\n"
      ],
      "metadata": {
        "id": "czESxIvxaZVi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}